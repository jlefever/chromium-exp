Usage: write_tables (--graphstore spec | --entries path) --out path
Creates a combined xrefs/filetree/search serving table based on a given GraphStore or stream of GraphStore-ordered entries

Version: v0.0.59 [Clean d72961d7d08938131cca04dce7c0951bc03c7fd4]

Flags:
  -async
    	Do not wait for job completion.
  -autoscaling_algorithm string
    	Autoscaling mode to use (optional).
  -beam_internal_sharding value
    	Controls how database keys are sharded in memory during processing. If the beam pipeline is running out of memory, use this to increase parallelism. Can be specified repeatedly for more control over shard computation. For example, if specified with -beam_internal_sharding 16 -beam_internal_sharding 4, the beam pipeline can use up to 16 machines to compute intermediate sharding information, then up to 4, then 1 to produce the final output. If unspecified, all database keys will be combined on a single machine to compute LevelDB shards.
  -beam_k int
    	Amount of memory to use when determining level DB shards. This will keep at most approximately O(k*log(n/k)) database keys in memory at once on a single machine, where n represents the total number of keys written to the database. As this value increases, elements will be distributed more evenly between the shards. By default, this will be set to the number of shards.
  -beam_shards int
    	Number of shards for beam processing. If non-positive, a reasonable default will be chosen.
  -beam_strict
    	Apply additional validation to pipelines.
  -compact_table
    	Whether to compact the output LevelDB after its creation
  -compress_shards
    	Determines whether intermediate data written to disk should be compressed.
  -control_endpoint string
    	Local control gRPC endpoint (required in worker mode).
  -cpu_profile string
    	Write CPU profile to the specified file (if nonempty)
  -cpu_profiling string
    	Job records CPU profiles to this GCS location (optional)
  -dataflow_endpoint string
    	Dataflow endpoint (optional).
  -dataflow_worker_jar string
    	Dataflow worker jar (optional)
  -disk_size_gb int
    	Size of root disk for VMs, in GB (optional).
  -dot_file string
    	DOT output file to create
  -dry_run
    	Dry run. Just print the job, but don't submit it.
  -endpoint string
    	Job service endpoint (required).
  -entries string
    	In non-beam mode: path to GraphStore-ordered entries file (mutually exclusive with --graphstore).
    	In beam mode: path to an unordered entries file, or if ending with slash, a directory containing such files.
  -environment_config string
    	Set environment configuration for running the user code.
    	For DOCKER: Url for the docker image.
    	For PROCESS: json of the form {"os": "<OS>", "arch": "<ARCHITECTURE>", "command": "<process to execute>", "env":{"<Environment variables 1>": "<ENV_VAL>"} }. All fields in the json are optional except command.
  -environment_type string
    	Environment Type. Possible options are DOCKER, and LOOPBACK. (default "DOCKER")
  -execute_async
    	Asynchronous execution. Submit the job and return immediately.
  -experimental_beam_columnar_data
    	Whether to emit columnar data from the Beam pipeline implementation
  -experimental_beam_pipeline
    	Whether to use the Beam experimental pipeline implementation
  -experimental_cross_reference_indirection_kinds value
    	Comma-separated set of key-value pairs (node_kind=edge_kind) to indirect through in CrossReferences.  For example, "talias=/kythe/edge/aliases" indicates that the targets of a 'talias' node's '/kythe/edge/aliases' related nodes will have their cross-references merged into the root 'talias' node's.  A "*=edge_kind" entry indicates to indirect through the specified edge kind for any node kind.
  -experimental_default_totals_quality string
    	Default TotalsQuality when unspecified in CrossReferencesRequest (default "APPROXIMATE_TOTALS")
  -experiments string
    	Comma-separated list of experiments (optional).
  -graphstore value
    	GraphStore to read (mutually exclusive with --entries) (default <nil>)
  -id string
    	Local identifier (required in worker mode).
  -job_name string
    	Job name (optional).
  -labels string
    	JSON-formatted map[string]string of job labels (optional).
  -logging_endpoint string
    	Local logging gRPC endpoint (required in worker mode).
  -max_num_workers int
    	Maximum number of workers during scaling (optional).
  -max_page_size int
    	If positive, edge/cross-reference pages are restricted to under this number of edges/references (default 4000)
  -max_shard_size int
    	Maximum number of elements (edges, decoration fragments, etc.) to keep in-memory before flushing an intermediary data shard to disk. (default 32000)
  -merge_cross_references
    	Whether to merge nodes when responding to a CrossReferencesRequest (default true)
  -min_cpu_platform string
    	GCE minimum cpu platform (optional)
  -network string
    	GCP network (optional)
  -no_use_public_ips
    	Workers must not use public IP addresses (optional)
  -num_workers int
    	Number of workers (optional).
  -options string
    	JSON-encoded pipeline options (required in worker mode).
  -out string
    	Directory path to output serving table
  -parallelism int
    	The degree of parallelism to be used when distributing operations onto Flink workers. (default -1)
  -project string
    	Google Cloud Platform project ID.
  -region string
    	GCP Region (required)
  -retain_docker_containers
    	Retain Docker containers created by the runner.
  -runner string
    	Pipeline runner. (default "direct")
  -sdk_harness_container_image_override value
    	Overrides for SDK harness container images. Could be for the local SDK or for a remote SDK that pipeline has to support due to a cross-language transform. Each entry consists of two values separated by a comma where first value gives a regex to identify the container image to override and the second value gives the replacement container image. Multiple entries can be specified by using this flag multiple times. A container will have no more than 1 override applied to it. If multiple overrides match a container image it is arbitrary which will be applied.
  -semi_persist_dir string
    	Local semi-persistent directory (optional in worker mode). (default "/tmp")
  -service_account_email string
    	Service account email (optional).
  -session_recording string
    	Job records session transcripts
  -staging_location string
    	GCS staging location (required).
  -status_endpoint string
    	Local status gRPC endpoint (optional in worker mode).
  -subnetwork string
    	GCP subnetwork (optional)
  -teardown_policy string
    	Job teardown policy (internal only).
  -temp_location string
    	Temp location (optional)
  -verbose
    	Whether to emit extra, and possibly excessive, log messages
  -verbose_disksort_runner
    	Log verbose info from the disksort runner
  -worker
    	Whether binary is running in worker mode.
  -worker_binary string
    	Worker binary (optional)
  -worker_harness_container_image string
    	Worker harness container image (required).
  -worker_machine_type string
    	GCE machine type (optional)
  -worker_region string
    	Dataflow worker region (optional)
  -worker_zone string
    	Dataflow worker zone (optional)
  -zone string
    	GCP zone (optional)
